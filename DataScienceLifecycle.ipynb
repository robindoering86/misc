{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of Project 2: Kickstarter Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<\n",
    "## 1) Business Understanding\n",
    "\n",
    "* Stakeholders: Entrepreneurs planning to run a Kickstarter project\n",
    "* Key Assignment: Help stakeholders predict whether project will be successful before they launch & invest too many ressources\n",
    "* Future Work: Predict a realistic pledge goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Mining\n",
    "\n",
    "* Dataset was provided as 50 csv files\n",
    "* Load and concatenate into one dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data Cleaning\n",
    "\n",
    "* Project required extensive data cleaning:\n",
    "    * various features with missing values\n",
    "    * various unbalanced features \n",
    "* mulitple columns feature stored as a string reprensentation of a dictionary\n",
    "* extraction of these data from these columns was time consuming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Data Exploration\n",
    "\n",
    "* Standard desciptive statistics on features\n",
    "* Visuals: mainly bar plots\n",
    "* Histograms, comparison with normal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Feature Engineering\n",
    "\n",
    "* different ideas were discussed\n",
    "* many were beyond the scope of the project\n",
    "* 'base pledge', that is pledged amount divided by the number of backers turned out to be a very important feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Predictive Modeling\n",
    "\n",
    "* 4 different models were used\n",
    "* Logistic Regression: very simple, but poor results\n",
    "* Decision Trees: slightly improved results\n",
    "* Random Forests: good results, easy to implement, cheap\n",
    "* xgboost: computationally expensive, results only marginally better than RF with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Data Visualization\n",
    "\n",
    "* A lot of insight was gained by looking at success rates of KS projects for different categories\n",
    "* Heatmaps were helpful in understanding correlations of features with target variables and multi-collinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BU'></a>\n",
    "# I business understanding\n",
    "\n",
    "**A) steps**\n",
    "\n",
    "a. Business setting\n",
    "\n",
    "b. Problem statement\n",
    "\n",
    "c. Business values\n",
    "\n",
    "d. knowing time frame\n",
    "\n",
    "e. ressources: time, people, finances, energy/prioritization, hw/sw equipment\n",
    "\n",
    "f. metric of success, e.g. focus on low false positive (precision), low false negative (recall)\n",
    "\n",
    "**B) hows**\n",
    "\n",
    "1. Ask biz stakeholders\n",
    "\n",
    "2. Own research google/scholar\n",
    "\n",
    "3. company website\n",
    "\n",
    "4. Competitor website(s)\n",
    "\n",
    "5. Company intranet / wiki\n",
    "\n",
    "6. arxiv\n",
    "\n",
    "7. What other data scientists have done in same or similar domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II Data mining\n",
    "\n",
    "CSV, json\n",
    "SQL \n",
    "PySpark\n",
    "webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III Data Cleaning (after initial look at data)\n",
    "\n",
    "initial look\n",
    "\n",
    "a) understanding the features (column names / predictor variables)\n",
    "    -> figuring out what the dependennt variable / predicted is\n",
    "    \n",
    "b) understanding looking at rows / observations for each column / feature\n",
    "\n",
    "c) Look at shape / size of the data is it\n",
    "    too large / too small (or just wright)?\n",
    "    \n",
    "<font color='red'>\n",
    "if too small: ask for or get more data\n",
    "\n",
    "synthetic data generation if imbalanced data set\n",
    "\n",
    "if too large: should we random sample? \n",
    "</font>\n",
    "\n",
    "d) is data loaded correctly? Example: zip codes starting with a zero\n",
    "\n",
    "e) do we have multiple datasets?\n",
    "combine them / truncate them / modify them?\n",
    "\n",
    "f) checking data type\n",
    "\n",
    "g) are data on same scale: timezone / currency\n",
    "\n",
    "h) missing values, how are they encoded? not always are they encoded as \"NaN\" or \"None\", sometimees string \"missing\" \"-999\" etc.\n",
    "\n",
    "i) Checking for duplicates, e.g. say rows of observations duplicated\n",
    "\n",
    "j) is there an unique identifier column\n",
    "\n",
    "k) Check for inconsistencies, e.g. in housing dataset, house with 33 bedrooms, while is possible, appears unlikely\n",
    "\n",
    "l) Check for irrelevant: check relevancy of data with problem statement e.g. Drug dataset had a fictional drug that doesnt exist\n",
    "\n",
    "m) Check for sub/additional datasets, say complete project info is found from *csv *json, dataset, webscraping etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Renaming removing whitespaces / characters\n",
    "\n",
    "a1) Convert types, e.g. if features should a date but is actually a string, convert it accordingly\n",
    "\n",
    "b) Handle missing data:\n",
    "    impute, i.e. data mean/median etc of column\n",
    "    delete the feature\n",
    "    delete observations\n",
    "    insert a dummy for missing values, e.g. for a categorical variable, we can make an extra dummy column for missing values\n",
    "    talk to the business\n",
    "    \n",
    "  For houseprices project, ew saw we have multicollinear features with missing values so this was telling us, that DELETING it was not best options\n",
    "    \n",
    "c) Look for similiar or same data\n",
    "    too large --> \n",
    "    \n",
    "    rows: take random sample (after cleaning & \n",
    " \n",
    "    columns/features: do PCA (as part of feature engineering)\n",
    "\n",
    "d) is data loaded correctly? depending on initial look, we correct the feature values\n",
    "    correct zipcode values that make sense, substitute -9999 with actual zipcodes (with leading 0)\n",
    "    \n",
    "e) Find relationships between datasets & combine them\n",
    "\n",
    "f) (i) E.g. applied pandas to_datetime() method to convert string to timestamp\n",
    "   (ii) str reprensentation of dict --> convert str to dict\n",
    "   (iii) Try to convert to appropriate type, e.g. check if can be converted to numerical variable \n",
    "   \n",
    "g) Categorical data can be encoded differently, e.g. 0/1; yes/no. ensure that for same data feature we encode them using the same manner.\n",
    "\n",
    "h \n",
    "\n",
    "i\n",
    "\n",
    "j) Handling of duplicates is use case specific!\n",
    "    Remove duplicates \n",
    "    For anomaly detection, we would keep duplicates as they can help identify patterns\n",
    "    \n",
    "\n",
    "k) Example of 33 bedrooms --> research it or make an assumption that the correct entry was supposed to be 3. \n",
    "Always articulate and comment in Notebook!\n",
    "\n",
    "l) Please look \n",
    "\n",
    "m) Refer to f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV Data Exploration\n",
    "\n",
    "a) Summary statistics (pandas.DataFrame.describe()), tells us distributions of numerical features/ variables\n",
    "\n",
    "b) To support (a) above, visualize the numerical features (matplotlib, seaborn, bokeh, folium), plot distribution of y variable\n",
    "\n",
    "c) Plot the distribution of categorical variables as bar plot or pie plot\n",
    "\n",
    "d) Explore target / predicted variable, relationship with predictor variables --> scatterplots,\n",
    "d1) Check for correlation, do heatmap\n",
    "\n",
    "e) Define distinct queries to get more granular insights on data (SQL)\n",
    "\n",
    "f) Check outliers from distribution plots (or scatter plots)\n",
    "\n",
    "g) Make more advanced plot (e.g. plot the success rate of female entrepreneurs by category) to dive deeper into understanding the various features relationships among each other & with the predicted variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Feature Engineering\n",
    "\n",
    "a) Transforming skewed continous variables to more normally distributed variables (log(1+x)-transform). \n",
    "\n",
    "b) Extract new features from features, say from name feature extract gender\n",
    "b1) Create new features by combining existing features, e.g. from total sales and number as purchases calculate average size of purchase.\n",
    "\n",
    "c) Create new categories by either:\n",
    "    binning numericals variables into categories\n",
    "    regrouping old categories into new categories\n",
    "    \n",
    "d) Scaling data\n",
    "    \n",
    "e) Create dummy variables for variables we want to use\n",
    "\n",
    "f) Dimensionalty reduction, e.g. PCA or KMeansClustering\n",
    "\n",
    "g) Impute values for features (* this can be a part of data cleaning as well as feature engineering)\n",
    "\n",
    "h) Assign weights to features, denoting feature **importance**\n",
    "\n",
    "i) Remove undesired features, drop them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI Predictive Modeling\n",
    "\n",
    "a) Pick models to use that are appropriate for the current challenge + ressources identified in \"business understanding\"\n",
    "e.g. in Kaggle competitions winners use stacking as a way to get best performance / evaluation results. In practical world, this is usually not the case.\n",
    "\n",
    "b) train test split (90-10, 70-30, 80-20)\n",
    "\n",
    "c) What is the baseline model? E.g. guessing in the case of a binary classification problem. \n",
    "Based on predetermined metric of success from <a href='#BU'>B.U.</a> determine which of the evaluation metric. \n",
    "\n",
    "e) Run the model\n",
    "        - K-fold cross validation\n",
    "        - Grid-Search\n",
    "\n",
    "f) Evaluate model performance:\n",
    "        - confusion matrix\n",
    "        - classification report\n",
    "\n",
    "g) Compare models (ideally at least against evaluation metrics and baseline model)\n",
    "\n",
    "h) Indentifying the most important features across all models.\n",
    "\n",
    "i) * Hypothesis testin for marketing / sales / healthcare domains\n",
    "\n",
    "j) Visualize the model output based on most imported features, e.g. model predicted probalitties against actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VII Data Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ((ii) create dummies for categorical variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
